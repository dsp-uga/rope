\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Formatting instructions for NIPS 2018}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
 The growing tourism industry demands that tourists be provided with immediate information about the places they visit.
 To address this issue we provide a framework which aims to detect and correctly recognize a landmark from images taken by tourists. 
 We test our model on the google landmark recognition challenge hosted on kaggle. 
 In the google landmark recognition challenge hosted on kaggle. 
 Unlike previous challenges, this task requires classification of 15k classes. We demonstrate the efficacy of our models by training it on the given set of images with ground truth and testing it on unlabelled dataset.
 We obtain xyz score on the kaggle leader board on the testing set.  
 
\end{abstract}

\section{Introduction}

Neural networks have made a lot of progress in recent years owing to the presence of multiple image classification dataset like  {\cite{imagenet},\cite{cifar10},cifar{1000}}.
Many architectures have been designed to specifically address these problems. Networks such as VGG16 {\cite{VGG}} and AlexNet {\cite{Alexnet}} have set the premise for more research.
In this project we compare the performance of traditional networks such as the VGG-16  with that of more sophisticated designs such as customized DenseNets adopted from the works of {\cite {huang_densely}} and demonstrate that a customized DenseNet works better.
We further utilize the concept of ensemble learning, by using a multi-pathway Neural network where one of the inputs use clustering information of all the images, and a third branch which is pretrained on the image net dataset. These along with the features extracted form the raw images demonstrate the best performance obtaining a score of xyz on the Kaggle leaderboard.



\subsection{Method}

A neural network based classifier such as the VGG-16 is normally used when it comes to image classification. The standard VGG-16 architecture {~\figurename{fig1}} consists of 
multiple convolution blocks of same filter size followed by a pooling layer to downsample the image by half along both the axis.The first two blocks contain two convolution layers each with filters of size 64 and 128 respectively followed by pooling layers. The third, fourth and fifth branches contain three layers of filters of size 256, 512, and 512. Following this the original VGG paper consists of fully connected layers. 

In order to reduce the number of parameter in the above architecture, we replace the fully connected layers with convolution layers of filter size (1x1). This has two improvement; firstly it reduces the number of parameter drastically since convolution layers share parameters. Secondly the utilization of convolution layers ensure that the spacial relation of the pixels are taken into account during the classification.

We also investigate deeper networks, namely a 36 layer densely connected convolution network. Motivated by the densely connected architecture of {\cite{huang_densely}} where every layer is connected to every other layer in a block, our version of dense network consists of five densely connected block. The first block consists of four convolution layers with 12 feature maps in each block except for the last, which consists of 36 layers. The second till fifth block also consists of 12 convolution filters in each of the seven layers except on the eight layer which increases as 64, 128, 256, 512 for each of the ultimate layer of a dense block. We also introduce batch normalization {\cite{batch_norm}} in order to ensure that the relu activation layers do not blow out of proportion. {\figurename{fig2}}.

Our intuition suggests that the performance of our classifier can be improved by using hierarchical classification first before fine tuning those features. We anticipate that landmarks which are water based will be different from those which are land based and so on. Thus we cluster our 15000 classes into 100 clusters. This number was chosen arbitrarily and we would like to experiment more on this later. Thus we send to our 36 layer dense-net, the cluster information that image belongs to along with the raw image.

One of the challenges with the task at hand is that many of the images are dominated by faces of the person taking the picture {\figurename{fig3-picsofhumanfaces}} and other objects. This makes the classification task harder since the image contains a lot of noise which we would like to remove. 

In order to overcome this issue, we decided to introduce pretrained network to generate a probability map of the contents of the image. We use a pretrained VGG-16, trained on the Image-Net dataset. The probability map output by this network is used to improve our classification accuracy. The intuition is that some landmarks, local features may dominate which can help identify them. With these two modification we observe that our performance improves by xyz.

 
\subsection{Experiments}

\subsubsection{Dataset}

We evaluate our methods on the images of the Google Landmark recognition challenge hosted on kaggle.{\cite{landmarkchallenge}}
There are 1225029 images in the training set belonging to 15k classes and 117703 testing images whose classes we are to predict. The dataset consists of hashes and URL key mapped to them. The images are of different sizes and resolutions. We use a down loader script from {\cite{source}}. In total we ended up with x images since some of the links were not active.
We experimented with different image resolution and decided to use 64x64 since the performance didnt vary much for greater resolution. The input to our networks, and the clustering algorithm were images resized to 64x64.
\subsubsection{Experiment Settings}
We used Keras {\cite{keras}} with tensorflow backend {\cite{tensorflow}}to implement our networks. To overcome the overfitting issue we used dropout rate of 0.3 and we also use L2 regularizer to penalize network
weights with large magnitudes and its control hyperparameter Î» is set to
2e-4. For training, we use the Adam optimizer {\cite{adam}} with a learning rate 1e-3. All
networks run up to 10 epochs.
For our clustering algorithm @Omid enter here
1
1
1
1
For our pretraining model @Parya enter here
1
1
1
1
\subsubsection{Experiment Results}
1
1
1
1
1
1
1
1


\begin{table}
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\section*{References}

References follow the acknowledgments. Use unnumbered first-level
heading for the references. Any choice of citation style is acceptable
as long as you are consistent. It is permissible to reduce the font
size to \verb+small+ (9 point) when listing the references. {\bf
  Remember that you can use more than eight pages as long as the
  additional pages contain \emph{only} cited references.}
\medskip

\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
  Exploring Realistic Neural Models with the GEneral NEural SImulation
  System.}  New York: TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
learning and recall at excitatory recurrent synapses and cholinergic
modulation in rat hippocampal region CA3. {\it Journal of
  Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
